{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forth Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "from dlpmln import DeepLPMLN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from digit_network import Net, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dprogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the DeepLPMLN program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprogram='''\n",
    "%1{num1(0, X): X=0..9}1.\n",
    "%1{num2(0, X): X=0..9}1.\n",
    "%1{carry(0,X): X=0..1}1.\n",
    "\n",
    "nn(m1(A, B, Carry, 1), result, [0,1,2,3,4,5,6,7,8,9]) :- num1(P,A), num2(P,B), Carry=0..1.\n",
    "nn(m2(A, B, Carry, 1), carry, [0,1]) :- num1(P,A), num2(P,B), Carry=0..1.\n",
    "\n",
    "result(P,X) :- num1(P, A), num2(P, B), carry(P, Carry), result(A,B,Carry,0,X).\n",
    "carry(P+1,X) :- num1(P, A), num2(P, B), carry(P, Carry), carry(A,B,Carry,0,X).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neural network class that will be used for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "\n",
    "    def __init__(self, *sizes):\n",
    "        super(FC, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes)-2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        layers.append(nn.Softmax(1))\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Instantiation\n",
    "- Instantiate neural networks.\n",
    "- Map neural networks into the NNMapping function called 'function'.\n",
    "- Specify the optimizers for each network (we use the Adam optimizer here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = FC(30,25,10) # network for adding the numbers\n",
    "m2 = FC(30,5,2)   # network for finding the carry out\n",
    "\n",
    "\n",
    "functions = {'m1':m1, 'm2':m2} #NNMapping\n",
    "\n",
    "optimizers = {'m1':torch.optim.Adam(m1.parameters(), lr=0.05),'m2':torch.optim.Adam(m2.parameters(), lr=0.05)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_data_sample() \n",
    "Creates a single sample of raw data which will later be turned into a data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sample():\n",
    "    \n",
    "    \n",
    "    add1=[np.random.randint(0,10),np.random.randint(0,10)]\n",
    "    add2=[np.random.randint(0,10),np.random.randint(0,10)]\n",
    "    \n",
    "    carry=np.random.randint(0,2)\n",
    "    \n",
    "    #breakpoint()\n",
    "    \n",
    "    num1=int(str(add1[0])+str(add1[1]))\n",
    "    num2=int(str(add2[0])+str(add2[1]))\n",
    "    num_sum=num1+num2+carry\n",
    "    num_sum_str=str(num_sum)\n",
    "    \n",
    "    if num_sum_str.__len__()==3:\n",
    "        numstr1=num_sum_str[0]\n",
    "        numstr2=num_sum_str[1]\n",
    "        numstr3=num_sum_str[2]\n",
    "    elif num_sum_str.__len__()==2:\n",
    "        numstr1=0\n",
    "        numstr2=num_sum_str[0]\n",
    "        numstr3=num_sum_str[1]\n",
    "    elif num_sum_str.__len__()==1:\n",
    "        numstr1=0\n",
    "        numstr2=0\n",
    "        numstr3=num_sum_str[0]\n",
    "    \n",
    "    \n",
    "    rule= 'add([{0},{1}],[{2},{3}],{4},[{5},{6},{7}]).'.format(add1[0],add1[1],add2[0],add2[1],carry,numstr1,numstr2,numstr3)\n",
    "    #print(rule)\n",
    "    \n",
    "    return rule,[add1[0],add1[1],add2[0],add2[1],carry,numstr1,numstr2,numstr3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 20 samples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add([1,7],[7,6],1,[0,9,4]).\n",
      "add([3,1],[8,7],1,[1,1,9]).\n",
      "add([9,4],[7,9],1,[1,7,4]).\n",
      "add([6,9],[1,2],1,[0,8,2]).\n",
      "add([2,9],[5,1],0,[0,8,0]).\n",
      "add([3,9],[1,6],1,[0,5,6]).\n",
      "add([4,4],[2,2],0,[0,6,6]).\n",
      "add([6,7],[3,5],1,[1,0,3]).\n",
      "add([7,8],[6,2],1,[1,4,1]).\n",
      "add([1,5],[5,7],0,[0,7,2]).\n",
      "add([4,2],[2,6],0,[0,6,8]).\n",
      "add([6,6],[2,8],0,[0,9,4]).\n",
      "add([7,2],[7,8],0,[1,5,0]).\n",
      "add([3,0],[7,9],1,[1,1,0]).\n",
      "add([8,8],[5,3],1,[1,4,2]).\n",
      "add([0,1],[5,0],1,[0,5,2]).\n",
      "add([2,1],[8,9],1,[1,1,1]).\n",
      "add([3,8],[3,7],1,[0,7,6]).\n",
      "add([4,7],[4,1],0,[0,8,8]).\n",
      "add([6,4],[9,1],1,[1,5,6]).\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(create_data_sample()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format_dataList() \n",
    "Returns a single dictionary of four data pointers and their corresponding Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataList(obs,str_list):\n",
    "    \n",
    "    \n",
    "    #breakpoint()\n",
    "    \n",
    "    key1='{0},{1},{2}'.format(str_list[1],str_list[3],0)\n",
    "    key2='{0},{1},{2}'.format(str_list[1],str_list[3],1)\n",
    "    key3='{0},{1},{2}'.format(str_list[0],str_list[2],0)\n",
    "    key4='{0},{1},{2}'.format(str_list[0],str_list[2],1)\n",
    "    \n",
    "    \n",
    "    n_digits=10\n",
    "    size=3\n",
    "    \n",
    "    y=torch.zeros(size,n_digits)\n",
    "\n",
    "    x=torch.LongTensor(size,1).random_()%n_digits\n",
    "    x[0,0]=5\n",
    "    y.scatter_(1,x,1)\n",
    "    \n",
    "    #breakpoint()\n",
    "    DATA1_idx=torch.LongTensor([[str_list[1]],[str_list[3]],[0]])\n",
    "    DATA2_idx=torch.LongTensor([[str_list[1]],[str_list[3]],[1]])\n",
    "    DATA3_idx=torch.LongTensor([[str_list[0]],[str_list[2]],[0]])\n",
    "    DATA4_idx=torch.LongTensor([[str_list[0]],[str_list[2]],[1]])\n",
    "    \n",
    "    \n",
    "    DATA1=torch.zeros(size,n_digits)\n",
    "    DATA2=torch.zeros(size,n_digits)\n",
    "    DATA3=torch.zeros(size,n_digits)\n",
    "    DATA4=torch.zeros(size,n_digits)\n",
    "    \n",
    "    # DATA1=DATA1.scatter_(1,DATA1_idx,1).view(30)\n",
    "    # DATA2=DATA2.scatter_(1,DATA2_idx,1).view(30)\n",
    "    # DATA3=DATA3.scatter_(1,DATA3_idx,1).view(30)\n",
    "    # DATA4=DATA4.scatter_(1,DATA4_idx,1).view(30)\n",
    "\n",
    "    DATA1=DATA1.scatter_(1,DATA1_idx,1).view(1,30)\n",
    "    DATA2=DATA2.scatter_(1,DATA2_idx,1).view(1,30)\n",
    "    DATA3=DATA3.scatter_(1,DATA3_idx,1).view(1,30)\n",
    "    DATA4=DATA4.scatter_(1,DATA4_idx,1).view(1,30)\n",
    "    \n",
    "    #breakpoint()\n",
    "    dataList_dict={key1:DATA1,key2:DATA2,key3:DATA3,key4:DATA4}\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataList_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format_observations()\n",
    "A function for creating the string containing ASP constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_observations(obs,str_list):\n",
    "    \n",
    "    obs_string= '''\n",
    "    \n",
    "    num1(1,{0}).\n",
    "    num1(0,{1}).\n",
    "    \n",
    "    num2(1,{2}).\n",
    "    num2(0,{3}).\n",
    "    \n",
    "    carry(0,{4}).\n",
    "    \n",
    "    :-not carry(2,{5}).\n",
    "    :-not result(1,{6}).\n",
    "    :-not result(0,{7}).\n",
    "    '''.format(str_list[0],str_list[1],str_list[2],str_list[3],str_list[4],str_list[5],str_list[6],str_list[7])\n",
    "    \n",
    "    return obs_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset classes for testing m1 and m2 networks\n",
    "Pytorch Dataset class used to create the test data for neural network 'm1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_test(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,size):\n",
    "        self.size=size\n",
    "        self.data=[]\n",
    "        self.create_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \n",
    "        return self.data[idx]\n",
    "    \n",
    "    def create_data(self):\n",
    "        #breakpoint()\n",
    "        for i in range(self.size):\n",
    "            obs,str_list=create_data_sample()\n",
    "        \n",
    "            data_list_sample=format_dataList(obs,str_list)\n",
    "            \n",
    "            label_value=str_list[1]+str_list[3] +str_list[4]\n",
    "            label_value=int(str(label_value)[-1])\n",
    "            \n",
    "            \n",
    "            keys=[]\n",
    "            for key in data_list_sample.keys():\n",
    "                keys.append(key)\n",
    "    \n",
    "            \n",
    "            #breakpoint()\n",
    "            if len(keys)<4:\n",
    "                #breakpoint()\n",
    "                \n",
    "                if str_list[4]==0:\n",
    "                    key = keys[0]\n",
    "                else:\n",
    "                    key = keys[1]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if str_list[4]==0:\n",
    "                    key = keys[0]\n",
    "                else:\n",
    "                    key = keys[1]\n",
    "            \n",
    "    \n",
    "            \n",
    "            x = data_list_sample[key]\n",
    "            \n",
    "            y_value=int(str_list[-1])\n",
    "            \n",
    "            \n",
    "            output_size=10\n",
    "    \n",
    "            \n",
    "            y=torch.zeros(1,output_size)\n",
    "            #breakpoint()\n",
    "            d=torch.LongTensor(1,1).random_()%output_size\n",
    "            d[0,0]=label_value\n",
    "            y.scatter_(1,d,1)\n",
    "            y=y.squeeze()\n",
    "            y=y.argmax(dim=0,keepdim=True)\n",
    "            #breakpoint()\n",
    "            self.data.append([x.squeeze(),y])\n",
    "\n",
    "        return None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class carry_test(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,size):\n",
    "        self.size=size\n",
    "        self.data=[]\n",
    "        self.create_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \n",
    "        return self.data[idx]\n",
    "    \n",
    "    def create_data(self):\n",
    "        #breakpoint()\n",
    "        for i in range(self.size):\n",
    "            obs,str_list=create_data_sample()\n",
    "        \n",
    "            data_list_sample=format_dataList(obs,str_list)\n",
    "            \n",
    "            sum_value=str_list[1]+str_list[3] +str_list[4]\n",
    "            \n",
    "            if len(str(sum_value))==1:\n",
    "                label_value=0\n",
    "            else:\n",
    "                label_value=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            keys=[]\n",
    "            for key in data_list_sample.keys():\n",
    "                keys.append(key)\n",
    "    \n",
    "            \n",
    "            #breakpoint()\n",
    "            if len(keys)<4:\n",
    "                #breakpoint()\n",
    "                \n",
    "                if str_list[4]==0:\n",
    "                    key = keys[0]\n",
    "                else:\n",
    "                    key = keys[1]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if str_list[4]==0:\n",
    "                    key = keys[0]\n",
    "                else:\n",
    "                    key = keys[1]\n",
    "            \n",
    "    \n",
    "            \n",
    "            x = data_list_sample[key]\n",
    "            \n",
    "            y_value=int(str_list[-1])\n",
    "            \n",
    "            \n",
    "            output_size=2\n",
    "    \n",
    "            \n",
    "            y=torch.zeros(1,output_size)\n",
    "            #breakpoint()\n",
    "            d=torch.LongTensor(1,1).random_()%output_size\n",
    "            d[0,0]=label_value\n",
    "            y.scatter_(1,d,1)\n",
    "            y=y.squeeze()\n",
    "            y=y.argmax(dim=0,keepdim=True)\n",
    "            #breakpoint()\n",
    "            self.data.append([x.squeeze(),y])\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "add([8,2],[9,9],1,[1,8,2]).\n",
      "\n",
      "\n",
      "{'2,9,0': tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), '2,9,1': tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]), '8,9,0': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), '8,9,1': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])}\n",
      "\n",
      "    \n",
      "    num1(1,8).\n",
      "    num1(0,2).\n",
      "    \n",
      "    num2(1,9).\n",
      "    num2(0,9).\n",
      "    \n",
      "    carry(0,1).\n",
      "    \n",
      "    :-not carry(2,1).\n",
      "    :-not result(1,8).\n",
      "    :-not result(0,2).\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n')\n",
    "\n",
    "obs,str_list=create_data_sample() #data sample the same as DeepProbLog used \n",
    "print(obs+'\\n\\n')\n",
    "\n",
    "dl=format_dataList(obs,str_list) # dataList item\n",
    "print(dl)\n",
    "\n",
    "o=format_observations(obs,str_list) # obsList item\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataList and obsList \n",
    "\n",
    "This the code that will use format_dataList() and format_observations() to build the data set that our DeepLPMLN object will train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = []\n",
    "obsList = []\n",
    "\n",
    "train_size=5000\n",
    "test_size=1000\n",
    "\n",
    "\n",
    "for i in range(train_size):\n",
    "    \n",
    "    obs,str_list=create_data_sample()\n",
    "    \n",
    "    dataList.append(format_dataList(obs,str_list))\n",
    "    obsList.append(format_observations(obs,str_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test dataset and dataLoader for neural network \"m1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_test_dataset=add_test(test_size)\n",
    "\n",
    "add_test_dataloader=DataLoader(add_test_dataset,batch_size=2,shuffle=True)\n",
    "\n",
    "carry_test_dataset=carry_test(test_size)\n",
    "\n",
    "carry_test_dataloader=DataLoader(add_test_dataset,batch_size=4,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DeepLPMLN object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlpmlnObj = DeepLPMLN(dprogram, functions, optimizers, dynamicMVPP=True)\n",
    "#dlpmlnObj.device='cpu' #put the training on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Training for epoch 1 ...\n",
      "Training for epoch 2 ...\n",
      "Training for epoch 3 ...\n",
      "Test Accuracy on NN Only for m1: 14%\n",
      "Test Accuracy on NN Only for m2: 36%\n",
      "--- train time: 44.86467432975769 seconds ---\n",
      "--- test time: 0.4487729072570801 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "\n",
    "\n",
    "time1 = time.time()\n",
    "dlpmlnObj.learn(dataList=dataList, obsList=obsList, epoch=1)\n",
    "time2 = time.time()\n",
    "dlpmlnObj.testNN(\"m1\", add_test_dataloader) #test m1 network\n",
    "dlpmlnObj.testNN(\"m2\", carry_test_dataloader) #test m2 network\n",
    "print(\"--- train time: %s seconds ---\" % (time2 - time1))\n",
    "print(\"--- test time: %s seconds ---\" % (time.time() - time2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
